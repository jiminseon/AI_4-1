{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zq3jPR1o_hIU",
        "outputId": "0e1c08cc-4b7a-4ac6-dce8-06864896d612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.24692, saving model to ./model/01-0.2469.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 0.24692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 4: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 7: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 11: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 12: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 13: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 14: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 15: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 16: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 18: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 19: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 21: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 22: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 23: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 24: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 25: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 28: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 29: val_loss did not improve from 0.24692\n",
            "\n",
            "Epoch 30: val_loss improved from 0.24692 to 0.24659, saving model to ./model/30-0.2466.hdf5\n",
            "\n",
            "Epoch 31: val_loss improved from 0.24659 to 0.24484, saving model to ./model/31-0.2448.hdf5\n",
            "\n",
            "Epoch 32: val_loss improved from 0.24484 to 0.24310, saving model to ./model/32-0.2431.hdf5\n",
            "\n",
            "Epoch 33: val_loss improved from 0.24310 to 0.24146, saving model to ./model/33-0.2415.hdf5\n",
            "\n",
            "Epoch 34: val_loss improved from 0.24146 to 0.23988, saving model to ./model/34-0.2399.hdf5\n",
            "\n",
            "Epoch 35: val_loss improved from 0.23988 to 0.23837, saving model to ./model/35-0.2384.hdf5\n",
            "\n",
            "Epoch 36: val_loss improved from 0.23837 to 0.23697, saving model to ./model/36-0.2370.hdf5\n",
            "\n",
            "Epoch 37: val_loss improved from 0.23697 to 0.23578, saving model to ./model/37-0.2358.hdf5\n",
            "\n",
            "Epoch 38: val_loss improved from 0.23578 to 0.23467, saving model to ./model/38-0.2347.hdf5\n",
            "\n",
            "Epoch 39: val_loss improved from 0.23467 to 0.23366, saving model to ./model/39-0.2337.hdf5\n",
            "\n",
            "Epoch 40: val_loss improved from 0.23366 to 0.23276, saving model to ./model/40-0.2328.hdf5\n",
            "\n",
            "Epoch 41: val_loss improved from 0.23276 to 0.23195, saving model to ./model/41-0.2320.hdf5\n",
            "\n",
            "Epoch 42: val_loss improved from 0.23195 to 0.23124, saving model to ./model/42-0.2312.hdf5\n",
            "\n",
            "Epoch 43: val_loss improved from 0.23124 to 0.23059, saving model to ./model/43-0.2306.hdf5\n",
            "\n",
            "Epoch 44: val_loss improved from 0.23059 to 0.22999, saving model to ./model/44-0.2300.hdf5\n",
            "\n",
            "Epoch 45: val_loss improved from 0.22999 to 0.22944, saving model to ./model/45-0.2294.hdf5\n",
            "\n",
            "Epoch 46: val_loss improved from 0.22944 to 0.22894, saving model to ./model/46-0.2289.hdf5\n",
            "\n",
            "Epoch 47: val_loss improved from 0.22894 to 0.22845, saving model to ./model/47-0.2285.hdf5\n",
            "\n",
            "Epoch 48: val_loss improved from 0.22845 to 0.22797, saving model to ./model/48-0.2280.hdf5\n",
            "\n",
            "Epoch 49: val_loss improved from 0.22797 to 0.22748, saving model to ./model/49-0.2275.hdf5\n",
            "\n",
            "Epoch 50: val_loss improved from 0.22748 to 0.22696, saving model to ./model/50-0.2270.hdf5\n",
            "\n",
            "Epoch 51: val_loss improved from 0.22696 to 0.22640, saving model to ./model/51-0.2264.hdf5\n",
            "\n",
            "Epoch 52: val_loss improved from 0.22640 to 0.22578, saving model to ./model/52-0.2258.hdf5\n",
            "\n",
            "Epoch 53: val_loss improved from 0.22578 to 0.22510, saving model to ./model/53-0.2251.hdf5\n",
            "\n",
            "Epoch 54: val_loss improved from 0.22510 to 0.22435, saving model to ./model/54-0.2243.hdf5\n",
            "\n",
            "Epoch 55: val_loss improved from 0.22435 to 0.22354, saving model to ./model/55-0.2235.hdf5\n",
            "\n",
            "Epoch 56: val_loss improved from 0.22354 to 0.22267, saving model to ./model/56-0.2227.hdf5\n",
            "\n",
            "Epoch 57: val_loss improved from 0.22267 to 0.22174, saving model to ./model/57-0.2217.hdf5\n",
            "\n",
            "Epoch 58: val_loss improved from 0.22174 to 0.22075, saving model to ./model/58-0.2207.hdf5\n",
            "\n",
            "Epoch 59: val_loss improved from 0.22075 to 0.21970, saving model to ./model/59-0.2197.hdf5\n",
            "\n",
            "Epoch 60: val_loss improved from 0.21970 to 0.21861, saving model to ./model/60-0.2186.hdf5\n",
            "\n",
            "Epoch 61: val_loss improved from 0.21861 to 0.21751, saving model to ./model/61-0.2175.hdf5\n",
            "\n",
            "Epoch 62: val_loss improved from 0.21751 to 0.21639, saving model to ./model/62-0.2164.hdf5\n",
            "\n",
            "Epoch 63: val_loss improved from 0.21639 to 0.21528, saving model to ./model/63-0.2153.hdf5\n",
            "\n",
            "Epoch 64: val_loss improved from 0.21528 to 0.21418, saving model to ./model/64-0.2142.hdf5\n",
            "\n",
            "Epoch 65: val_loss improved from 0.21418 to 0.21307, saving model to ./model/65-0.2131.hdf5\n",
            "\n",
            "Epoch 66: val_loss improved from 0.21307 to 0.21198, saving model to ./model/66-0.2120.hdf5\n",
            "\n",
            "Epoch 67: val_loss improved from 0.21198 to 0.21089, saving model to ./model/67-0.2109.hdf5\n",
            "\n",
            "Epoch 68: val_loss improved from 0.21089 to 0.20984, saving model to ./model/68-0.2098.hdf5\n",
            "\n",
            "Epoch 69: val_loss improved from 0.20984 to 0.20884, saving model to ./model/69-0.2088.hdf5\n",
            "\n",
            "Epoch 70: val_loss improved from 0.20884 to 0.20791, saving model to ./model/70-0.2079.hdf5\n",
            "\n",
            "Epoch 71: val_loss improved from 0.20791 to 0.20702, saving model to ./model/71-0.2070.hdf5\n",
            "\n",
            "Epoch 72: val_loss improved from 0.20702 to 0.20618, saving model to ./model/72-0.2062.hdf5\n",
            "\n",
            "Epoch 73: val_loss improved from 0.20618 to 0.20539, saving model to ./model/73-0.2054.hdf5\n",
            "\n",
            "Epoch 74: val_loss improved from 0.20539 to 0.20461, saving model to ./model/74-0.2046.hdf5\n",
            "\n",
            "Epoch 75: val_loss improved from 0.20461 to 0.20386, saving model to ./model/75-0.2039.hdf5\n",
            "\n",
            "Epoch 76: val_loss improved from 0.20386 to 0.20311, saving model to ./model/76-0.2031.hdf5\n",
            "\n",
            "Epoch 77: val_loss improved from 0.20311 to 0.20234, saving model to ./model/77-0.2023.hdf5\n",
            "\n",
            "Epoch 78: val_loss improved from 0.20234 to 0.20156, saving model to ./model/78-0.2016.hdf5\n",
            "\n",
            "Epoch 79: val_loss improved from 0.20156 to 0.20076, saving model to ./model/79-0.2008.hdf5\n",
            "\n",
            "Epoch 80: val_loss improved from 0.20076 to 0.19993, saving model to ./model/80-0.1999.hdf5\n",
            "\n",
            "Epoch 81: val_loss improved from 0.19993 to 0.19908, saving model to ./model/81-0.1991.hdf5\n",
            "\n",
            "Epoch 82: val_loss improved from 0.19908 to 0.19817, saving model to ./model/82-0.1982.hdf5\n",
            "\n",
            "Epoch 83: val_loss improved from 0.19817 to 0.19721, saving model to ./model/83-0.1972.hdf5\n",
            "\n",
            "Epoch 84: val_loss improved from 0.19721 to 0.19620, saving model to ./model/84-0.1962.hdf5\n",
            "\n",
            "Epoch 85: val_loss improved from 0.19620 to 0.19516, saving model to ./model/85-0.1952.hdf5\n",
            "\n",
            "Epoch 86: val_loss improved from 0.19516 to 0.19407, saving model to ./model/86-0.1941.hdf5\n",
            "\n",
            "Epoch 87: val_loss improved from 0.19407 to 0.19297, saving model to ./model/87-0.1930.hdf5\n",
            "\n",
            "Epoch 88: val_loss improved from 0.19297 to 0.19189, saving model to ./model/88-0.1919.hdf5\n",
            "\n",
            "Epoch 89: val_loss improved from 0.19189 to 0.19083, saving model to ./model/89-0.1908.hdf5\n",
            "\n",
            "Epoch 90: val_loss improved from 0.19083 to 0.18982, saving model to ./model/90-0.1898.hdf5\n",
            "\n",
            "Epoch 91: val_loss improved from 0.18982 to 0.18887, saving model to ./model/91-0.1889.hdf5\n",
            "\n",
            "Epoch 92: val_loss improved from 0.18887 to 0.18799, saving model to ./model/92-0.1880.hdf5\n",
            "\n",
            "Epoch 93: val_loss improved from 0.18799 to 0.18716, saving model to ./model/93-0.1872.hdf5\n",
            "\n",
            "Epoch 94: val_loss improved from 0.18716 to 0.18632, saving model to ./model/94-0.1863.hdf5\n",
            "\n",
            "Epoch 95: val_loss improved from 0.18632 to 0.18540, saving model to ./model/95-0.1854.hdf5\n",
            "\n",
            "Epoch 96: val_loss improved from 0.18540 to 0.18445, saving model to ./model/96-0.1844.hdf5\n",
            "\n",
            "Epoch 97: val_loss improved from 0.18445 to 0.18347, saving model to ./model/97-0.1835.hdf5\n",
            "\n",
            "Epoch 98: val_loss improved from 0.18347 to 0.18245, saving model to ./model/98-0.1825.hdf5\n",
            "\n",
            "Epoch 99: val_loss improved from 0.18245 to 0.18140, saving model to ./model/99-0.1814.hdf5\n",
            "\n",
            "Epoch 100: val_loss improved from 0.18140 to 0.18033, saving model to ./model/100-0.1803.hdf5\n",
            "\n",
            "Epoch 101: val_loss improved from 0.18033 to 0.17929, saving model to ./model/101-0.1793.hdf5\n",
            "\n",
            "Epoch 102: val_loss improved from 0.17929 to 0.17826, saving model to ./model/102-0.1783.hdf5\n",
            "\n",
            "Epoch 103: val_loss improved from 0.17826 to 0.17729, saving model to ./model/103-0.1773.hdf5\n",
            "\n",
            "Epoch 104: val_loss improved from 0.17729 to 0.17639, saving model to ./model/104-0.1764.hdf5\n",
            "\n",
            "Epoch 105: val_loss improved from 0.17639 to 0.17556, saving model to ./model/105-0.1756.hdf5\n",
            "\n",
            "Epoch 106: val_loss improved from 0.17556 to 0.17480, saving model to ./model/106-0.1748.hdf5\n",
            "\n",
            "Epoch 107: val_loss improved from 0.17480 to 0.17410, saving model to ./model/107-0.1741.hdf5\n",
            "\n",
            "Epoch 108: val_loss improved from 0.17410 to 0.17336, saving model to ./model/108-0.1734.hdf5\n",
            "\n",
            "Epoch 109: val_loss improved from 0.17336 to 0.17256, saving model to ./model/109-0.1726.hdf5\n",
            "\n",
            "Epoch 110: val_loss improved from 0.17256 to 0.17173, saving model to ./model/110-0.1717.hdf5\n",
            "\n",
            "Epoch 111: val_loss improved from 0.17173 to 0.17084, saving model to ./model/111-0.1708.hdf5\n",
            "\n",
            "Epoch 112: val_loss improved from 0.17084 to 0.16989, saving model to ./model/112-0.1699.hdf5\n",
            "\n",
            "Epoch 113: val_loss improved from 0.16989 to 0.16890, saving model to ./model/113-0.1689.hdf5\n",
            "\n",
            "Epoch 114: val_loss improved from 0.16890 to 0.16787, saving model to ./model/114-0.1679.hdf5\n",
            "\n",
            "Epoch 115: val_loss improved from 0.16787 to 0.16686, saving model to ./model/115-0.1669.hdf5\n",
            "\n",
            "Epoch 116: val_loss improved from 0.16686 to 0.16590, saving model to ./model/116-0.1659.hdf5\n",
            "\n",
            "Epoch 117: val_loss improved from 0.16590 to 0.16475, saving model to ./model/117-0.1647.hdf5\n",
            "\n",
            "Epoch 118: val_loss improved from 0.16475 to 0.16357, saving model to ./model/118-0.1636.hdf5\n",
            "\n",
            "Epoch 119: val_loss improved from 0.16357 to 0.16244, saving model to ./model/119-0.1624.hdf5\n",
            "\n",
            "Epoch 120: val_loss improved from 0.16244 to 0.16107, saving model to ./model/120-0.1611.hdf5\n",
            "\n",
            "Epoch 121: val_loss improved from 0.16107 to 0.15946, saving model to ./model/121-0.1595.hdf5\n",
            "\n",
            "Epoch 122: val_loss improved from 0.15946 to 0.15765, saving model to ./model/122-0.1577.hdf5\n",
            "\n",
            "Epoch 123: val_loss improved from 0.15765 to 0.15577, saving model to ./model/123-0.1558.hdf5\n",
            "\n",
            "Epoch 124: val_loss improved from 0.15577 to 0.15406, saving model to ./model/124-0.1541.hdf5\n",
            "\n",
            "Epoch 125: val_loss improved from 0.15406 to 0.15280, saving model to ./model/125-0.1528.hdf5\n",
            "\n",
            "Epoch 126: val_loss improved from 0.15280 to 0.15193, saving model to ./model/126-0.1519.hdf5\n",
            "\n",
            "Epoch 127: val_loss improved from 0.15193 to 0.15158, saving model to ./model/127-0.1516.hdf5\n",
            "\n",
            "Epoch 128: val_loss did not improve from 0.15158\n",
            "\n",
            "Epoch 129: val_loss did not improve from 0.15158\n",
            "\n",
            "Epoch 130: val_loss did not improve from 0.15158\n",
            "\n",
            "Epoch 131: val_loss did not improve from 0.15158\n",
            "\n",
            "Epoch 132: val_loss did not improve from 0.15158\n",
            "\n",
            "Epoch 133: val_loss improved from 0.15158 to 0.15114, saving model to ./model/133-0.1511.hdf5\n",
            "\n",
            "Epoch 134: val_loss improved from 0.15114 to 0.15044, saving model to ./model/134-0.1504.hdf5\n",
            "\n",
            "Epoch 135: val_loss improved from 0.15044 to 0.14968, saving model to ./model/135-0.1497.hdf5\n",
            "\n",
            "Epoch 136: val_loss improved from 0.14968 to 0.14901, saving model to ./model/136-0.1490.hdf5\n",
            "\n",
            "Epoch 137: val_loss improved from 0.14901 to 0.14849, saving model to ./model/137-0.1485.hdf5\n",
            "\n",
            "Epoch 138: val_loss improved from 0.14849 to 0.14822, saving model to ./model/138-0.1482.hdf5\n",
            "\n",
            "Epoch 139: val_loss improved from 0.14822 to 0.14817, saving model to ./model/139-0.1482.hdf5\n",
            "\n",
            "Epoch 140: val_loss did not improve from 0.14817\n",
            "\n",
            "Epoch 141: val_loss did not improve from 0.14817\n",
            "\n",
            "Epoch 142: val_loss did not improve from 0.14817\n",
            "\n",
            "Epoch 143: val_loss did not improve from 0.14817\n",
            "\n",
            "Epoch 144: val_loss improved from 0.14817 to 0.14779, saving model to ./model/144-0.1478.hdf5\n",
            "\n",
            "Epoch 145: val_loss improved from 0.14779 to 0.14724, saving model to ./model/145-0.1472.hdf5\n",
            "\n",
            "Epoch 146: val_loss improved from 0.14724 to 0.14663, saving model to ./model/146-0.1466.hdf5\n",
            "\n",
            "Epoch 147: val_loss improved from 0.14663 to 0.14602, saving model to ./model/147-0.1460.hdf5\n",
            "\n",
            "Epoch 148: val_loss improved from 0.14602 to 0.14549, saving model to ./model/148-0.1455.hdf5\n",
            "\n",
            "Epoch 149: val_loss improved from 0.14549 to 0.14505, saving model to ./model/149-0.1450.hdf5\n",
            "\n",
            "Epoch 150: val_loss improved from 0.14505 to 0.14468, saving model to ./model/150-0.1447.hdf5\n",
            "\n",
            "Epoch 151: val_loss improved from 0.14468 to 0.14435, saving model to ./model/151-0.1443.hdf5\n",
            "\n",
            "Epoch 152: val_loss improved from 0.14435 to 0.14402, saving model to ./model/152-0.1440.hdf5\n",
            "\n",
            "Epoch 153: val_loss improved from 0.14402 to 0.14366, saving model to ./model/153-0.1437.hdf5\n",
            "\n",
            "Epoch 154: val_loss improved from 0.14366 to 0.14323, saving model to ./model/154-0.1432.hdf5\n",
            "\n",
            "Epoch 155: val_loss improved from 0.14323 to 0.14274, saving model to ./model/155-0.1427.hdf5\n",
            "\n",
            "Epoch 156: val_loss improved from 0.14274 to 0.14225, saving model to ./model/156-0.1423.hdf5\n",
            "\n",
            "Epoch 157: val_loss improved from 0.14225 to 0.14176, saving model to ./model/157-0.1418.hdf5\n",
            "\n",
            "Epoch 158: val_loss improved from 0.14176 to 0.14131, saving model to ./model/158-0.1413.hdf5\n",
            "\n",
            "Epoch 159: val_loss improved from 0.14131 to 0.14090, saving model to ./model/159-0.1409.hdf5\n",
            "\n",
            "Epoch 160: val_loss improved from 0.14090 to 0.14054, saving model to ./model/160-0.1405.hdf5\n",
            "\n",
            "Epoch 161: val_loss improved from 0.14054 to 0.14034, saving model to ./model/161-0.1403.hdf5\n",
            "\n",
            "Epoch 162: val_loss improved from 0.14034 to 0.14000, saving model to ./model/162-0.1400.hdf5\n",
            "\n",
            "Epoch 163: val_loss improved from 0.14000 to 0.13961, saving model to ./model/163-0.1396.hdf5\n",
            "\n",
            "Epoch 164: val_loss improved from 0.13961 to 0.13935, saving model to ./model/164-0.1393.hdf5\n",
            "\n",
            "Epoch 165: val_loss improved from 0.13935 to 0.13913, saving model to ./model/165-0.1391.hdf5\n",
            "\n",
            "Epoch 166: val_loss improved from 0.13913 to 0.13901, saving model to ./model/166-0.1390.hdf5\n",
            "\n",
            "Epoch 167: val_loss improved from 0.13901 to 0.13874, saving model to ./model/167-0.1387.hdf5\n",
            "\n",
            "Epoch 168: val_loss improved from 0.13874 to 0.13846, saving model to ./model/168-0.1385.hdf5\n",
            "\n",
            "Epoch 169: val_loss improved from 0.13846 to 0.13833, saving model to ./model/169-0.1383.hdf5\n",
            "\n",
            "Epoch 170: val_loss improved from 0.13833 to 0.13830, saving model to ./model/170-0.1383.hdf5\n",
            "\n",
            "Epoch 171: val_loss improved from 0.13830 to 0.13815, saving model to ./model/171-0.1382.hdf5\n",
            "\n",
            "Epoch 172: val_loss improved from 0.13815 to 0.13795, saving model to ./model/172-0.1379.hdf5\n",
            "\n",
            "Epoch 173: val_loss improved from 0.13795 to 0.13792, saving model to ./model/173-0.1379.hdf5\n",
            "\n",
            "Epoch 174: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 175: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 176: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 177: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 178: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 179: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 180: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 181: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 182: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 183: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 184: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 185: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 186: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 187: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 188: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 189: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 190: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 191: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 192: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 193: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 194: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 195: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 196: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 197: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 198: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 199: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 200: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 201: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 202: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 203: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 204: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 205: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 206: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 207: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 208: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 209: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 210: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 211: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 212: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 213: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 214: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 215: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 216: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 217: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 218: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 219: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 220: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 221: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 222: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 223: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 224: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 225: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 226: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 227: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 228: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 229: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 230: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 231: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 232: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 233: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 234: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 235: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 236: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 237: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 238: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 239: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 240: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 241: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 242: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 243: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 244: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 245: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 246: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 247: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 248: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 249: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 250: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 251: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 252: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 253: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 254: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 255: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 256: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 257: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 258: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 259: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 260: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 261: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 262: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 263: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 264: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 265: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 266: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 267: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 268: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 269: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 270: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 271: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 272: val_loss did not improve from 0.13792\n",
            "\n",
            "Epoch 273: val_loss did not improve from 0.13792\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# seed 값 설정\n",
        "seed = 0\n",
        "numpy.random.seed(seed)\n",
        "tf.random.set_seed(3)\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/colab/deeplearning/dataset/sonar.csv', header=None)\n",
        "\n",
        "'''\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "'''\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y_obj = dataset[:,60]\n",
        "\n",
        "e = LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj).astype(float)\n",
        "\n",
        "# 학습 셋과 테스트 셋의 구분\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(24,  input_dim=60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "            optimizer='adam',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "# 모델 저장 폴더 만들기\n",
        "MODEL_DIR = './model/'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "   os.mkdir(MODEL_DIR)\n",
        "\n",
        "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "\n",
        "# 모델 업데이트 및 저장\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# 학습 자동 중단 설정\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_split=0.1, epochs=3500, batch_size=500, verbose=0, callbacks=[early_stopping_callback,checkpointer])\n",
        "\n",
        "# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
        "y_vloss=history.history['val_loss']\n",
        "\n",
        "# y_vacc 에 테스트셋으로 측정한 정확도의 값을 저장\n",
        "y_vacc=history.history['val_accuracy']\n",
        "\n",
        "# y_acc 에 학습 셋으로 측정한 정확도의 값을 저장\n",
        "y_acc=history.history['accuracy']\n",
        "\n",
        "# x값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
        "x_len = numpy.arange(len(y_acc))\n",
        "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
        "plt.plot(x_len, y_vacc, \"o\", c=\"yellow\", markersize=3)\n",
        "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
        "\n",
        "plt.show()"
      ]
    }
  ]
}