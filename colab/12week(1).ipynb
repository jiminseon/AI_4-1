{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOxm7ssgF-kp",
        "outputId": "d73fa4d1-7b1c-4f29-d933-88b260401790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            title  label\n",
            "0       버거킹 바삭한 신메뉴 3000원도 안되는 가격      1\n",
            "1        버거킹 가을 맞이 와퍼주니어 1900원 판매      0\n",
            "2  귀엽거나 오싹하거나’할로윈데이 겨냥한 이색 식음료 출시      1\n",
            "3    버거킹 몬스터X의 할로윈 에디션 스크림몬스터X 출시      1\n",
            "4  햄버거 브랜드 버거킹 치킨 사이드 신메뉴 바삭킹 선보여      1\n",
            " 첫번째 입력:  버거킹 바삭한 신메뉴 3000원도 안되는 가격\n",
            " 첫번째 one-hot 출력:  [0. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "from keras.models import Sequential\n",
        "#from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.layers import Dense, Flatten, Embedding\n",
        "\n",
        "#seed 값 설정\n",
        "numpy.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/인공지능/12week/train_mydataset_6000.csv', header=0, names=['title', 'label'])\n",
        "print(train_data.head(5))\n",
        "X_train = train_data['title']\n",
        "Y_train = train_data['label']\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/인공지능/12week/test_mydataset_1500.csv', header=0)\n",
        "X_test = test_data['title']\n",
        "Y_test = test_data['label']\n",
        "\n",
        "# 데이터 확인하기\n",
        "print(\" 첫번째 입력: \", X_train[0])\n",
        "Y_train_onehot = to_categorical(Y_train)\n",
        "Y_test_onehot = to_categorical(Y_test)\n",
        "print(\" 첫번째 one-hot 출력: \", Y_train_onehot[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = Tokenizer()\n",
        "token.fit_on_texts(X_train)\n",
        "#print(token.word_index)\n",
        "print(X_train[0])\n",
        "x_train = token.texts_to_sequences(X_train)\n",
        "print(\"첫번째 토큰 결과: \",  x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVGyLEAXJIOl",
        "outputId": "7eddbd1f-647c-4ed7-c299-b4048561b4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "버거킹 바삭한 신메뉴 3000원도 안되는 가격\n",
            "첫번째 토큰 결과:  [1, 2804, 41, 5168, 1512, 23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_max = max(len(item) for item in x_train)\n",
        "print(\"학습셋 제목 최대 길이: \", train_max)"
      ],
      "metadata": {
        "id": "KsvM2uslJ64U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ce587f-b794-444b-ba76-2a6632c18139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습셋 제목 최대 길이:  14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = token.texts_to_sequences(X_test)\n",
        "test_max = max(len(item) for item in x_test)\n",
        "print(\"테스트셋 제목 최대 길이: \", test_max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKLtya7wZPjU",
        "outputId": "b67f4efa-d13f-41e0-ca55-463b04904cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트셋 제목 최대 길이:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_train = pad_sequences(x_train, train_max)\n",
        "print(\"첫번째 패딩 토큰: \", padded_train[0])\n",
        "\n",
        "padded_test = pad_sequences(x_test, train_max)\n",
        "\n",
        "#딥러닝 모델\n",
        "print(\"\\n딥러닝 모델 시작:\")\n",
        "\n",
        "#임베딩에 입력될 단어의 수를 지정합니다.\n",
        "word_size = len(token.word_index) +1\n",
        "print(\"Word size: \", word_size)\n",
        "\n",
        "#단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_size, 8, input_length=train_max))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(padded_train, Y_train_onehot, epochs=20)\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_test, Y_test_onehot)[1]))"
      ],
      "metadata": {
        "id": "ibE6sK4kHfVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f59ade8-af09-4ac8-c734-b268a8598609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫번째 패딩 토큰:  [   0    0    0    0    0    0    0    0    1 2804   41 5168 1512   23]\n",
            "\n",
            "딥러닝 모델 시작:\n",
            "Word size:  12464\n",
            "Epoch 1/20\n",
            "188/188 [==============================] - 2s 4ms/step - loss: 0.8909 - accuracy: 0.6513\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.6606 - accuracy: 0.7212\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4134 - accuracy: 0.8462\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2395 - accuracy: 0.9470\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1420 - accuracy: 0.9773\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0886 - accuracy: 0.9918\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0588 - accuracy: 0.9962\n",
            "Epoch 8/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0407 - accuracy: 0.9975\n",
            "Epoch 9/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0295 - accuracy: 0.9988\n",
            "Epoch 10/20\n",
            "188/188 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9992\n",
            "Epoch 11/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0170 - accuracy: 0.9995\n",
            "Epoch 12/20\n",
            "188/188 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.9997\n",
            "Epoch 13/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0107 - accuracy: 0.9998\n",
            "Epoch 14/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0072 - accuracy: 0.9998\n",
            "Epoch 16/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.8453\n",
            "\n",
            " Accuracy: 0.8453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jed1IgyAcSaO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}